{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c55f405-9936-4f63-8377-152768fca9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          GN_ID Gene_Name  \\\n",
      "0   Apoa2_sp|P09813|APOA2_MOUSE     Apoa2   \n",
      "1     Hrnr_sp|Q8VHD8|HORN_MOUSE      Hrnr   \n",
      "2  Mup10_tr|A2BIN1|A2BIN1_MOUSE     Mup10   \n",
      "\n",
      "   log2FC(insoluble/soluble protein ratio)  -LOG10(P-VALUE)    Prefix  \\\n",
      "0                                 3.319557         1.927557  Apoa2_sp   \n",
      "1                                 2.918873         1.789432   Hrnr_sp   \n",
      "2                                 2.596492         2.066665  Mup10_tr   \n",
      "\n",
      "  UniProt_ID  UniProt_Name Direction  \n",
      "0     P09813   APOA2_MOUSE        up  \n",
      "1     Q8VHD8    HORN_MOUSE        up  \n",
      "2     A2BIN1  A2BIN1_MOUSE        up  \n",
      "(40, 8)\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# read files\n",
    "excel_file = \"../data/Murine_Protein_Solubility_Data/Demontis example_Up & down proteins for proteome enrichment.xlsx\"\n",
    "sheets = ['up', 'down']\n",
    "output_folder = '../data/Murine_Protein_Solubility_Data/processed/' \n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "dfs = []\n",
    "for sheet in sheets:\n",
    "    # read sheet\n",
    "    df = pd.read_excel(excel_file, sheet_name=sheet)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    # split GN_ID into parts\n",
    "    split_cols = df['GN_ID'].str.split('|', expand=True)\n",
    "    df['Prefix'] = split_cols[0]\n",
    "    df['UniProt_ID'] = split_cols[1]\n",
    "    df['UniProt_Name'] = split_cols[2]\n",
    "    # add direction col\n",
    "    df['Direction'] = sheet\n",
    "    # save\n",
    "    dfs.append(df)\n",
    "\n",
    "# combine up and down sheets\n",
    "all_proteins = pd.concat(dfs, ignore_index=True)\n",
    "print(all_proteins.head(3))\n",
    "all_proteins.to_csv(os.path.join(output_folder, 'all_enriched_proteins.csv'), index=False)\n",
    "\n",
    "# create a demo subset for test (N=20 for each direction)\n",
    "topN = 20\n",
    "log2fc_threshold = 1\n",
    "log10pval_threshold = 1.5\n",
    "\n",
    "up_proteins = (\n",
    "    all_proteins[\n",
    "        (all_proteins['Direction'] == 'up') &\n",
    "        (all_proteins['log2FC(insoluble/soluble protein ratio)'] > log2fc_threshold) &\n",
    "        (all_proteins['-LOG10(P-VALUE)'] > log10pval_threshold)\n",
    "    ]\n",
    "    .nlargest(topN, '-LOG10(P-VALUE)')\n",
    ")\n",
    "\n",
    "down_proteins = (\n",
    "    all_proteins[\n",
    "        (all_proteins['Direction'] == 'down') &\n",
    "        (all_proteins['log2FC(insoluble/soluble protein ratio)'] < -log2fc_threshold) &\n",
    "        (all_proteins['-LOG10(P-VALUE)'] > log10pval_threshold)\n",
    "    ]\n",
    "    .nlargest(topN, '-LOG10(P-VALUE)')\n",
    ")\n",
    "\n",
    "# concat list and save \n",
    "demo_proteins = pd.concat([up_proteins, down_proteins])\n",
    "print(demo_proteins.shape) \n",
    "demo_proteins.to_csv(os.path.join(output_folder, 'demo_proteins_for_download.csv'), index=False)\n",
    "demo_proteins[['UniProt_ID']].to_csv(os.path.join(output_folder, 'uniprot_ids_for_batch.txt'), index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121b24ad-1102-4863-9100-31a5065f53d3",
   "metadata": {},
   "source": [
    "## SCanchi Notes\n",
    "\n",
    "The above script reads the original excel sheet, extracts the data for both up and down proteins, splits the column that has uniprot ids and creates a demo protein list (top 20 for either direction) to be used for initial data acquisition. \n",
    "\n",
    "The next step is to obtain both the sequence data and structure data for the protein subset. UniProt offers [programmatic access](https://www.uniprot.org/api-documentation/idmapping) and includes mapping to other databases. However, I found out that the mapping does not actually download the files and that is a separate step. Since we need structure information and it is unlikely for PDB to have all, as a first step, I plan to get predicted structure from AlphaFold. Cross checking PDB for protein in the list for solved structres to compare against predicted structure from Alphafold can become an optional downstream step. Since AlphaFold offers the model organism proteome, choosing to download the entire set for local query instead of API calls for individual structures.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b36f415c-0b65-48fa-8bc4-3b1f783b8121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying UniProt with URL:\n",
      "https://rest.uniprot.org/uniprotkb/stream?format=fasta&query=(accession:P14069 OR accession:Q8C3X2 OR accession:P56812 OR accession:P22599 OR accession:P49817 OR accession:O08917 OR accession:P11588 OR accession:P11798 OR accession:E9QA79 OR accession:Q9Z2L6 OR accession:Q8BFZ9 OR accession:A0A075B6A3 OR accession:P56376 OR accession:Q8VHD8 OR accession:O70475 OR accession:Q91X78 OR accession:P07758 OR accession:Q9D0M1 OR accession:P01592 OR accession:Q00896 OR accession:P07759 OR accession:P97352 OR accession:P03977 OR accession:Q91W29 OR accession:P63300 OR accession:A2AQ43 OR accession:P09813 OR accession:Q64314 OR accession:A2BIN1 OR accession:Q9Z239 OR accession:Q6PD31 OR accession:Q8R574 OR accession:P12246 OR accession:Q00898 OR accession:P50543)\n",
      "FASTA seqeunces saved to ../data/Murine_Protein_Solubility_Data/processed/demo_protein_sequences.fasta\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# set paths\n",
    "base_dir = '../data/Murine_Protein_Solubility_Data/processed'\n",
    "id_file = os.path.join(base_dir, 'uniprot_ids_for_batch.txt')\n",
    "output_fasta = os.path.join(base_dir, 'demo_protein_sequences.fasta')\n",
    "\n",
    "# read the ids from file\n",
    "with open(id_file) as f:\n",
    "    ids = [line.strip().split('-')[0] for line in f if line.strip()]\n",
    "ids = list(set(ids))    \n",
    "\n",
    "# get ids as comma separated for api call\n",
    "query = \" OR \".join([f\"accession:{x}\" for x in ids])\n",
    "url = f\"https://rest.uniprot.org/uniprotkb/stream?format=fasta&query=({query})\"\n",
    "print(\"Querying UniProt with URL:\")\n",
    "print(url)\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(output_fasta, 'w') as fasta_out:\n",
    "        fasta_out.write(response.text)\n",
    "    print(f\"FASTA seqeunces saved to {output_fasta}\")\n",
    "else:\n",
    "    print(f\"Download failed with status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b11779-e302-4018-b54e-39692ec0d830",
   "metadata": {},
   "source": [
    "# SCanchi Notes\n",
    "\n",
    "I downloaded the latest mouse proteome from AlphaFold: `wget https://ftp.ebi.ac.uk/pub/databases/alphafold/latest/UP000000589_10090_MOUSE_v4.tar -P af_mouse_latest/`\n",
    "\n",
    "For the UniProt download, I initially ran in error code 400 (bad URL). Upon closer inspection, it turned out some of the UniProt IDs had isoform format (-1, -2 etc) which probably doesn't work for query URL. There were also duplicate entries between the up and down sets which most likely stems from collapsing of data across multiple experiments/groups etc. \n",
    "\n",
    "The next step is to run Aggrescan3D which helps calculate the aggregation propensity score given a protein structure. The reason the team decided to use the 2.0 version was since the latest 4.0 version includes pH aware calculation which might time for completion. I looked into the [documentation](https://biocomp.chem.uw.edu.pl/A3D2/) and they have both a local download and a rest API access. The local download requires python 2.7 but since their REST API is well documented, I decided to use that instead. Since the Aggrescan3D requires pdb files in uncompressed format, I decided to create a separate folder for the subset files. This also allowed to capture any missing structure files in the AF reference side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83484c39-2ef2-42ad-b280-e3993ee8e343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied AF-P14069-F1-model_v4.pdb.gz\n",
      "Copied AF-Q8C3X2-F1-model_v4.pdb.gz\n",
      "Copied AF-P56812-F1-model_v4.pdb.gz\n",
      "Copied AF-P22599-F1-model_v4.pdb.gz\n",
      "Copied AF-P49817-F1-model_v4.pdb.gz\n",
      "Copied AF-O08917-F1-model_v4.pdb.gz\n",
      "Copied AF-P11588-F1-model_v4.pdb.gz\n",
      "Copied AF-P11798-F1-model_v4.pdb.gz\n",
      "Copied AF-E9QA79-F1-model_v4.pdb.gz\n",
      "Copied AF-Q9Z2L6-F1-model_v4.pdb.gz\n",
      "Copied AF-Q8BFZ9-F1-model_v4.pdb.gz\n",
      "Not found: AF-A0A075B6A3-F1-model_v4.pdb.gz\n",
      "Copied AF-P56376-F1-model_v4.pdb.gz\n",
      "Copied AF-Q8VHD8-F1-model_v4.pdb.gz\n",
      "Copied AF-O70475-F1-model_v4.pdb.gz\n",
      "Copied AF-Q91X78-F1-model_v4.pdb.gz\n",
      "Copied AF-P07758-F1-model_v4.pdb.gz\n",
      "Copied AF-Q9D0M1-F1-model_v4.pdb.gz\n",
      "Copied AF-P01592-F1-model_v4.pdb.gz\n",
      "Copied AF-Q00896-F1-model_v4.pdb.gz\n",
      "Copied AF-P07759-F1-model_v4.pdb.gz\n",
      "Copied AF-P97352-F1-model_v4.pdb.gz\n",
      "Copied AF-P03977-F1-model_v4.pdb.gz\n",
      "Copied AF-Q91W29-F1-model_v4.pdb.gz\n",
      "Not found: AF-P63300-F1-model_v4.pdb.gz\n",
      "Not found: AF-A2AQ43-F1-model_v4.pdb.gz\n",
      "Copied AF-P09813-F1-model_v4.pdb.gz\n",
      "Copied AF-Q64314-F1-model_v4.pdb.gz\n",
      "Copied AF-A2BIN1-F1-model_v4.pdb.gz\n",
      "Copied AF-Q9Z239-F1-model_v4.pdb.gz\n",
      "Copied AF-Q6PD31-F1-model_v4.pdb.gz\n",
      "Copied AF-Q8R574-F1-model_v4.pdb.gz\n",
      "Copied AF-P12246-F1-model_v4.pdb.gz\n",
      "Copied AF-Q00898-F1-model_v4.pdb.gz\n",
      "Copied AF-P50543-F1-model_v4.pdb.gz\n",
      "Copied 32 files to ../data/Murine_Protein_Solubility_Data/processed/af_demo\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# set paths\n",
    "base_dir = '../data/Murine_Protein_Solubility_Data/processed'\n",
    "all_af = '../data/Murine_Protein_Solubility_Data/af_mouse_latest'\n",
    "id_file = os.path.join(base_dir, 'uniprot_ids_for_batch.txt')\n",
    "demo_dir = os.path.join(base_dir, 'af_demo')\n",
    "os.makedirs(demo_dir, exist_ok=True)\n",
    "\n",
    "# read the ids from file\n",
    "with open(id_file) as f:\n",
    "    ids = [line.strip().split('-')[0] for line in f if line.strip()]\n",
    "ids = list(set(ids))  \n",
    "\n",
    "# copy matching .pdb.gz files\n",
    "copied = 0\n",
    "for acc in ids:\n",
    "    af_filename = f\"AF-{acc}-F1-model_v4.pdb.gz\"\n",
    "    src = os.path.join(all_af, af_filename)\n",
    "    dst = os.path.join(demo_dir, af_filename)\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy(src, dst)\n",
    "        copied += 1\n",
    "        print(f\"Copied {af_filename}\")\n",
    "    else:\n",
    "        print(f\"Not found: {af_filename}\")\n",
    "print(f\"Copied {copied} files to {demo_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "004985dd-aead-41f4-903f-012704df876f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PDB files: 32\n",
      "Batch 1: 8 files\n",
      "Batch 2: 8 files\n",
      "Batch 3: 8 files\n",
      "Batch 4: 8 files\n",
      "\n",
      "Processing batch 1 of 4\n",
      "Submitting AF-A2BIN1-F1-model_v4.pdb... submitted as jobid=33d547779e8a66f. Waiting for completion.............. Done.\n",
      "Submitting AF-E9QA79-F1-model_v4.pdb... submitted as jobid=26290e19382496d. Waiting for completion............... Done.\n",
      "Submitting AF-O08917-F1-model_v4.pdb... submitted as jobid=b4adcf8900ae312. Waiting for completion................................... Done.\n",
      "Submitting AF-O70475-F1-model_v4.pdb... submitted as jobid=a8b3fa6843d54a8. Waiting for completion.................................. Done.\n",
      "Submitting AF-P01592-F1-model_v4.pdb... submitted as jobid=77534d5946deb9e. Waiting for completion.......... Done.\n",
      "Submitting AF-P03977-F1-model_v4.pdb... submitted as jobid=c09fa2892fb5896. Waiting for completion....... Done.\n",
      "Submitting AF-P07758-F1-model_v4.pdb... submitted as jobid=db233a618935e43. Waiting for completion......................... Done.\n",
      "Submitting AF-P07759-F1-model_v4.pdb... submitted as jobid=4db33cb4f56e017. Waiting for completion........................ Done.\n",
      "\n",
      "Processing batch 2 of 4\n",
      "Submitting AF-P09813-F1-model_v4.pdb... submitted as jobid=c792c1a38711a9f. Waiting for completion.......... Done.\n",
      "Submitting AF-P11588-F1-model_v4.pdb... submitted as jobid=504f68480dbf818. Waiting for completion.............. Done.\n",
      "Submitting AF-P11798-F1-model_v4.pdb... submitted as jobid=7aaa644cf26a141. Waiting for completion.................................. Done.\n",
      "Submitting AF-P12246-F1-model_v4.pdb... submitted as jobid=2d933d3021889df. Waiting for completion............... Done.\n",
      "Submitting AF-P14069-F1-model_v4.pdb... submitted as jobid=f20eab11a0aeb6. Waiting for completion............. Done.\n",
      "Submitting AF-P22599-F1-model_v4.pdb... submitted as jobid=c96937544f36ac4. Waiting for completion........................... Done.\n",
      "Submitting AF-P49817-F1-model_v4.pdb... submitted as jobid=871817869bf0902. Waiting for completion............ Done.\n",
      "Submitting AF-P50543-F1-model_v4.pdb... submitted as jobid=45dba739444050d. Waiting for completion........ Done.\n",
      "\n",
      "Processing batch 3 of 4\n",
      "Submitting AF-P56376-F1-model_v4.pdb... submitted as jobid=5294299499aa82a. Waiting for completion........ Done.\n",
      "Submitting AF-P56812-F1-model_v4.pdb... submitted as jobid=1939d92f5e6115. Waiting for completion.............. Done.\n",
      "Submitting AF-P97352-F1-model_v4.pdb... submitted as jobid=e17a72e8505ea84. Waiting for completion........... Done.\n",
      "Submitting AF-Q00896-F1-model_v4.pdb... submitted as jobid=8ca88f53e7effa8. Waiting for completion........................... Done.\n",
      "Submitting AF-Q00898-F1-model_v4.pdb... submitted as jobid=a1660ebc91845c6. Waiting for completion....................... Done.\n",
      "Submitting AF-Q64314-F1-model_v4.pdb... submitted as jobid=77262110581b009. Waiting for completion............... Done.\n",
      "Submitting AF-Q6PD31-F1-model_v4.pdb... submitted as jobid=c8d8cd60a6bf7cf. Waiting for completion................................................ Done.\n",
      "Submitting AF-Q8BFZ9-F1-model_v4.pdb... submitted as jobid=a8d148dff1e1971. Waiting for completion............................. Done.\n",
      "\n",
      "Processing batch 4 of 4\n",
      "Submitting AF-Q8C3X2-F1-model_v4.pdb... submitted as jobid=ea457241c7363e7. Waiting for completion........................ Done.\n",
      "Submitting AF-Q8R574-F1-model_v4.pdb... submitted as jobid=4e526c9f34d25b4. Waiting for completion....................... Done.\n",
      "Submitting AF-Q8VHD8-F1-model_v4.pdb... submitted as jobid=66f0b4414bbea3a. Waiting for completion............................................................ Done.\n",
      "Submitting AF-Q91W29-F1-model_v4.pdb... submitted as jobid=7000f48173214a. Waiting for completion............. Done.\n",
      "Submitting AF-Q91X78-F1-model_v4.pdb... submitted as jobid=b6df05c59583040. Waiting for completion............................ Done.\n",
      "Submitting AF-Q9D0M1-F1-model_v4.pdb... submitted as jobid=2936f4d2a0e3e4a. Waiting for completion...................... Done.\n",
      "Submitting AF-Q9Z239-F1-model_v4.pdb... submitted as jobid=b01a23b03a3b891. Waiting for completion......... Done.\n",
      "Submitting AF-Q9Z2L6-F1-model_v4.pdb... submitted as jobid=6965e05ce09e3c. Waiting for completion............................. Done.\n",
      "\n",
      "Saved results to ../data/Murine_Protein_Solubility_Data/processed/demo_aggrescan_scores_batches.csv\n"
     ]
    }
   ],
   "source": [
    "# load libraries\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# set paths\n",
    "base_dir = '../data/Murine_Protein_Solubility_Data/processed'\n",
    "pdb_dir = os.path.join(base_dir, 'af_demo')\n",
    "output_summary = os.path.join(base_dir, 'demo_aggrescan_scores_batches.csv')\n",
    "failure_log = os.path.join(base_dir, \"demo_aggrescan_failure_log.txt\")\n",
    "\n",
    "# calculate batch size and batch pdb files\n",
    "pdb_files = sorted(glob.glob(os.path.join(pdb_dir, \"*.pdb\")))\n",
    "num_batches = 4\n",
    "batches = np.array_split(pdb_files, num_batches)\n",
    "print(f\"Total PDB files: {len(pdb_files)}\")\n",
    "for i, batch in enumerate(batches):\n",
    "    print(f\"Batch {i+1}: {len(batch)} files\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# loop through each batch\n",
    "for batch_idx, batch in enumerate(batches):\n",
    "    print(f\"\\nProcessing batch {batch_idx+1} of {len(batches)}\")\n",
    "    for pdb_path in batch:\n",
    "        pdb_file = os.path.basename(pdb_path)\n",
    "        options = {\n",
    "            'dynamic': False,\n",
    "            'distance': 5,\n",
    "            'email': None,\n",
    "            'name': os.path.splitext(pdb_file)[0],\n",
    "            'hide': True,\n",
    "            'foldx': True,\n",
    "        }\n",
    "        # submit job to Aggrescan3D REST API\n",
    "        with open(pdb_path, 'rb') as fh:\n",
    "            files = {\n",
    "                'inputfile': (pdb_file, fh),\n",
    "                'json': (None, json.dumps(options), 'application/json')\n",
    "            }\n",
    "            print(f\"Submitting {pdb_file}...\", end='')\n",
    "            req = requests.post(\n",
    "                'https://biocomp.chem.uw.edu.pl/A3D2/RESTful/submit/userinput/',\n",
    "                files=files\n",
    "            )\n",
    "        if req.status_code != 200:\n",
    "            print(f\"[Failed: {req.status_code}]\")\n",
    "            all_results.append({'pdb': pdb_file, 'status': 'submission_failed'})\n",
    "            continue\n",
    "        jobid = req.json()['jobid']\n",
    "        print(f\" submitted as jobid={jobid}. Waiting for completion...\", end='')\n",
    "        \n",
    "        # poll for job completion (based on jobid status)\n",
    "        while True:\n",
    "            stat = requests.get(f'https://biocomp.chem.uw.edu.pl/A3D2/RESTful/job/{jobid}/status/')\n",
    "            status_str = stat.text.strip()\n",
    "            if 'done' in status_str:\n",
    "                print(\" Done.\")\n",
    "                break\n",
    "            elif 'error' in status_str:\n",
    "                print(f\" Error: {status_str}\")\n",
    "                all_results.append({'pdb': pdb_file, 'jobid': jobid, 'status': 'error'})\n",
    "                break\n",
    "            else:\n",
    "                print(\".\", end='', flush=True)\n",
    "                time.sleep(15)  \n",
    "\n",
    "        # get the aggregation scores if done\n",
    "        if 'done' in status_str:\n",
    "            r = requests.get(f'https://biocomp.chem.uw.edu.pl/A3D2/RESTful/job/{jobid}/')\n",
    "            try:\n",
    "                data = r.json()\n",
    "                # added support for key \"A3Dscore\"\n",
    "                if 'A3Dscore' in data:\n",
    "                    a3d = data['A3Dscore']\n",
    "                elif 'aggrescan3Dscore' in data:\n",
    "                    a3d = data['aggrescan3Dscore']\n",
    "                else:\n",
    "                    raise KeyError(\"No A3Dscore or aggrescan3Dscore in data\")\n",
    "                all_results.append({\n",
    "                    'pdb': pdb_file,\n",
    "                    'jobid': jobid,\n",
    "                    'status': 'done',\n",
    "                    'avg': a3d.get('avg', a3d.get('average_value', 'NA')),\n",
    "                    'sum': a3d.get('sum', a3d.get('summary_value', 'NA')),\n",
    "                    'min': a3d.get('min', a3d.get('min_value', 'NA')),\n",
    "                    'max': a3d.get('max', a3d.get('max_value', 'NA')),\n",
    "                })\n",
    "            except Exception as ex:\n",
    "                print(f\"\\nError parsing result for {jobid} ({pdb_file}): {ex}\")\n",
    "                with open(failure_log, \"a\") as log:\n",
    "                    log.write(f\"{pdb_file}  |  jobid: {jobid}\\n\")\n",
    "                    log.write(r.text)\n",
    "                    log.write(\"\\n\\n\" + \"=\"*60 + \"\\n\\n\")\n",
    "                all_results.append({'pdb': pdb_file, 'jobid': jobid, 'status': 'result_parse_failed'})\n",
    "\n",
    "# save results\n",
    "df = pd.DataFrame(all_results)\n",
    "df.to_csv(output_summary, index=False)\n",
    "print(f\"\\nSaved results to {output_summary}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c01e7e4-75c7-4ae3-89b9-133a9fa33a94",
   "metadata": {},
   "source": [
    "# SCanchi Notes\n",
    "\n",
    "I used the A3D [tutorials]https://biocomp.chem.uw.edu.pl/A3D2/tutorial) on use of REST API to script the submission and result download. The original example was for one file submission and I incorporated batch logic and result parsing to ensure the submission does not overwhelm the server. I initially ran into errors parsing file for every pdb file which made me suspect structure issues with AF files. But quick inpsection of the actual pdb files clarified that the file was structured correctly and in a recognizable format. I explored the actual server side error and noticed a key error. I started with `aggrescan3Dscore` but the key is now updated to `A3Dscore`. I also updated the error log to capture the full server side error message which will always be per file basis. \n",
    "\n",
    "## Future improvements\n",
    "\n",
    "The current implementation is still one file at a time submission with serial batching. For truly massive datasets, some parallel job submission balancing server loads is probably necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e28ce8-d352-4df6-b642-9ff1c230d8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_default",
   "language": "python",
   "name": "conda-env-py38_default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
